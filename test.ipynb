{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "*Transformer network with Multi-Head Attention*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatbot.transformer import Transformer\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/translation/dataset.csv', sep=';')\n",
    "tf = Transformer(lang='en-us', num_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded with hyperparams: Corpus(lang='en-us', max_length=15, batch_size=64, buffer_size=20000, _start_token=[10619], _end_token=[10620], _vocab_size=10621).\n",
      "<keras.engine.functional.Functional object at 0x0000015E3EDBE2F0> compiled successfully.\n",
      "Epoch 1/10\n",
      "347/347 [==============================] - 50s 101ms/step - loss: 2.9549 - _count_accuracy: 0.0762 - _count_f1: 0.0763\n",
      "Epoch 2/10\n",
      "347/347 [==============================] - 35s 101ms/step - loss: 1.8646 - _count_accuracy: 0.1461 - _count_f1: 0.1461\n",
      "Epoch 3/10\n",
      "347/347 [==============================] - 36s 103ms/step - loss: 1.6036 - _count_accuracy: 0.1720 - _count_f1: 0.1720\n",
      "Epoch 4/10\n",
      "347/347 [==============================] - 36s 102ms/step - loss: 1.4846 - _count_accuracy: 0.1822 - _count_f1: 0.1822\n",
      "Epoch 5/10\n",
      "347/347 [==============================] - 35s 100ms/step - loss: 1.4109 - _count_accuracy: 0.1880 - _count_f1: 0.1880\n",
      "Epoch 6/10\n",
      "347/347 [==============================] - 34s 97ms/step - loss: 1.3495 - _count_accuracy: 0.1933 - _count_f1: 0.1933\n",
      "Epoch 7/10\n",
      "347/347 [==============================] - 35s 100ms/step - loss: 1.2999 - _count_accuracy: 0.1968 - _count_f1: 0.1968\n",
      "Epoch 8/10\n",
      "347/347 [==============================] - 35s 101ms/step - loss: 1.2695 - _count_accuracy: 0.1973 - _count_f1: 0.1974\n",
      "Epoch 9/10\n",
      "347/347 [==============================] - 35s 100ms/step - loss: 1.2423 - _count_accuracy: 0.1991 - _count_f1: 0.1991\n",
      "Epoch 10/10\n",
      "347/347 [==============================] - 34s 98ms/step - loss: 1.2288 - _count_accuracy: 0.1986 - _count_f1: 0.1987\n"
     ]
    }
   ],
   "source": [
    "tf.fit(path='data/conv-ai-2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_models/transformer/conv-ai-2017/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_models/transformer/conv-ai-2017/assets\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000015E3C63ABF0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000015E3C639CC0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000015E3EC54E20> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000015E3EC6EA40> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000015E3CC7A470> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000015E3ECE1E70> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000015E45806680> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000015E45822E90> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000015E4589AB90> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000015E45872E60> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000015E45912710> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000015E459480D0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000015E45821D20> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000015E459E5390> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000015E458204F0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000015E45A5CA00> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000015E45A0BA60> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000015E45A0A050> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "tf.save_to_folder(path='trained_models/transformer/conv-ai-2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN\n",
    "\n",
    "*Bag-Of-Words based DNN network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.stem.lancaster import LancasterStemmer\n",
    "# import numpy as np\n",
    "# import tflearn\n",
    "# import tensorflow as tf\n",
    "# import json\n",
    "# import pickle\n",
    "# import random\n",
    "\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "# #Loading intents.json\n",
    "# with open('intents.json') as intents:\n",
    "#   data = json.load(intents)\n",
    "\n",
    "# stemmer = LancasterStemmer()\n",
    "\n",
    "# try:\n",
    "#     with open('data.pickle','rb') as f:\n",
    "#         words, labels, training, output = pickle.load(f)\n",
    "# except:\n",
    "# # Fetching and Feeding information--\n",
    "#     words = []\n",
    "#     labels = []\n",
    "#     x_docs = []\n",
    "#     y_docs = []\n",
    "\n",
    "#     for intent in data['intents']:\n",
    "#         for pattern in intent['patterns']:\n",
    "#             wrds = nltk.word_tokenize(pattern)\n",
    "#             words.extend(wrds)\n",
    "#             x_docs.append(wrds)\n",
    "#             y_docs.append(intent['tag'])\n",
    "\n",
    "#             if intent['tag'] not in labels:\n",
    "#                 labels.append(intent['tag'])\n",
    "\n",
    "#     words = [stemmer.stem(w.lower()) for w in words if w not in \"?\"]\n",
    "#     words = sorted(list(set(words)))\n",
    "#     labels = sorted(labels)\n",
    "\n",
    "#     training = []\n",
    "#     output = []\n",
    "\n",
    "#     out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "#     # One hot encoding, Converting the words to numerals\n",
    "#     for x, doc in enumerate(x_docs):\n",
    "#         bag = []\n",
    "#         wrds = [stemmer.stem(w) for w in doc]\n",
    "#         for w in words:\n",
    "#             if w in wrds:\n",
    "#                 bag.append(1)\n",
    "#             else:\n",
    "#                 bag.append(0)\n",
    "\n",
    "\n",
    "#         output_row = out_empty[:]\n",
    "#         output_row[labels.index(y_docs[x])] = 1\n",
    "\n",
    "#         training.append(bag)\n",
    "#         output.append(output_row)\n",
    "\n",
    "\n",
    "#     training = np.array(training)\n",
    "#     output = np.array(output)\n",
    "\n",
    "#     with open('data.pickle','wb') as f:\n",
    "#         pickle.dump((words, labels, training, output), f)\n",
    "\n",
    "\n",
    "# net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "# net = tflearn.fully_connected(net, 8)\n",
    "# net = tflearn.fully_connected(net, 8)\n",
    "# net = tflearn.fully_connected(net, len(output[0]), activation='softmax')\n",
    "# net = tflearn.regression(net)\n",
    "\n",
    "# model = tflearn.DNN(net)\n",
    "\n",
    "# try:\n",
    "#     model.load(\"model.tflearn\")\n",
    "# except:\n",
    "\n",
    "#     model.fit(training, output, n_epoch=100, batch_size=8, show_metric=True)\n",
    "#     model.save('model.tflearn')\n",
    "\n",
    "\n",
    "# def bag_of_words(s, words):\n",
    "#     bag = [0 for _ in range(len(words))]\n",
    "#     s_words = nltk.word_tokenize(s)\n",
    "#     s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "#     for se in s_words:\n",
    "#         for i, w in enumerate(words):\n",
    "#             if w == se:\n",
    "#                 bag[i] = 1\n",
    "\n",
    "#     return np.array(bag)\n",
    "\n",
    "\n",
    "# def chat():\n",
    "#     print(\"The bot is ready to talk!!(Type 'quit' to exit)\")\n",
    "#     while True:\n",
    "#         inp = input(\"\\nYou: \")\n",
    "#         if inp.lower() == 'quit':\n",
    "#             break\n",
    "\n",
    "#     #Porbability of correct response \n",
    "#         results = model.predict([bag_of_words(inp, words)])\n",
    "\n",
    "#     # Picking the greatest number from probability\n",
    "#         results_index = np.argmax(results)\n",
    "\n",
    "#         tag = labels[results_index]\n",
    "\n",
    "\n",
    "#         for tg in data['intents']:\n",
    "\n",
    "#             if tg['tag'] == tag:\n",
    "#                 responses = tg['responses']\n",
    "#             print(\"Bot:\" + random.choice(responses))\n",
    "\n",
    "\n",
    "# chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tfenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "891e4fc61ac792e208938f292df2fbd7ee5dda42dce8831cfe4bd29bcfe9707e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
