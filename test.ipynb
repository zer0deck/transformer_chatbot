{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "*Transformer network with Multi-Head Attention*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatbot.transformer import Transformer\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/translation/dataset.csv', sep=';')\n",
    "tf = Transformer(lang='en-us', num_epoch=100, max_length=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded with hyperparams: Corpus(lang='en-us', max_length=60, batch_size=64, buffer_size=20000, _start_token=[13691], _end_token=[13692], _vocab_size=13693).\n",
      "<keras.engine.functional.Functional object at 0x00000255796327D0> compiled successfully.\n",
      "Epoch 1/100\n",
      "433/433 [==============================] - 134s 273ms/step - loss: 0.9071 - _count_accuracy: 0.0202\n",
      "Epoch 2/100\n",
      "433/433 [==============================] - 113s 260ms/step - loss: 0.5739 - _count_accuracy: 0.0432\n",
      "Epoch 3/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.4925 - _count_accuracy: 0.0513\n",
      "Epoch 4/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.4580 - _count_accuracy: 0.0543\n",
      "Epoch 5/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.4350 - _count_accuracy: 0.0561\n",
      "Epoch 6/100\n",
      "433/433 [==============================] - 113s 260ms/step - loss: 0.4191 - _count_accuracy: 0.0572\n",
      "Epoch 7/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.4065 - _count_accuracy: 0.0579\n",
      "Epoch 8/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.3976 - _count_accuracy: 0.0584\n",
      "Epoch 9/100\n",
      "433/433 [==============================] - 113s 261ms/step - loss: 0.3938 - _count_accuracy: 0.0582\n",
      "Epoch 10/100\n",
      "433/433 [==============================] - 116s 267ms/step - loss: 0.3936 - _count_accuracy: 0.0577\n",
      "Epoch 11/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.3855 - _count_accuracy: 0.0582\n",
      "Epoch 12/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.3769 - _count_accuracy: 0.0588\n",
      "Epoch 13/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.3676 - _count_accuracy: 0.0595\n",
      "Epoch 14/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.3560 - _count_accuracy: 0.0603\n",
      "Epoch 15/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.3444 - _count_accuracy: 0.0613\n",
      "Epoch 16/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.3362 - _count_accuracy: 0.0619\n",
      "Epoch 17/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.3279 - _count_accuracy: 0.0628\n",
      "Epoch 18/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.3215 - _count_accuracy: 0.0634\n",
      "Epoch 19/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.3129 - _count_accuracy: 0.0645\n",
      "Epoch 20/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.3043 - _count_accuracy: 0.0654\n",
      "Epoch 21/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.2964 - _count_accuracy: 0.0666\n",
      "Epoch 22/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.2890 - _count_accuracy: 0.0677\n",
      "Epoch 23/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.2819 - _count_accuracy: 0.0688\n",
      "Epoch 24/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.2759 - _count_accuracy: 0.0696\n",
      "Epoch 25/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.2691 - _count_accuracy: 0.0709\n",
      "Epoch 26/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.2630 - _count_accuracy: 0.0718\n",
      "Epoch 27/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.2582 - _count_accuracy: 0.0727\n",
      "Epoch 28/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.2529 - _count_accuracy: 0.0736\n",
      "Epoch 29/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.2471 - _count_accuracy: 0.0747\n",
      "Epoch 30/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.2435 - _count_accuracy: 0.0752\n",
      "Epoch 31/100\n",
      "433/433 [==============================] - 113s 260ms/step - loss: 0.2377 - _count_accuracy: 0.0763\n",
      "Epoch 32/100\n",
      "433/433 [==============================] - 117s 270ms/step - loss: 0.2348 - _count_accuracy: 0.0769\n",
      "Epoch 33/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.2304 - _count_accuracy: 0.0777\n",
      "Epoch 34/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.2298 - _count_accuracy: 0.0779\n",
      "Epoch 35/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.2279 - _count_accuracy: 0.0783\n",
      "Epoch 36/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.2255 - _count_accuracy: 0.0789\n",
      "Epoch 37/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.2193 - _count_accuracy: 0.0801\n",
      "Epoch 38/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.2177 - _count_accuracy: 0.0803\n",
      "Epoch 39/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.2154 - _count_accuracy: 0.0809\n",
      "Epoch 40/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.2129 - _count_accuracy: 0.0814\n",
      "Epoch 41/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.2107 - _count_accuracy: 0.0819\n",
      "Epoch 42/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.2094 - _count_accuracy: 0.0821\n",
      "Epoch 43/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.2080 - _count_accuracy: 0.0827\n",
      "Epoch 44/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.2069 - _count_accuracy: 0.0829\n",
      "Epoch 45/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.2029 - _count_accuracy: 0.0837\n",
      "Epoch 46/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.2029 - _count_accuracy: 0.0838\n",
      "Epoch 47/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.1980 - _count_accuracy: 0.0848\n",
      "Epoch 48/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.1992 - _count_accuracy: 0.0846\n",
      "Epoch 49/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.1969 - _count_accuracy: 0.0851\n",
      "Epoch 50/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.1940 - _count_accuracy: 0.0859\n",
      "Epoch 51/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.1923 - _count_accuracy: 0.0863\n",
      "Epoch 52/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.1906 - _count_accuracy: 0.0867\n",
      "Epoch 53/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.1896 - _count_accuracy: 0.0869\n",
      "Epoch 54/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1877 - _count_accuracy: 0.0874\n",
      "Epoch 55/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.1877 - _count_accuracy: 0.0874\n",
      "Epoch 56/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1859 - _count_accuracy: 0.0878\n",
      "Epoch 57/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1829 - _count_accuracy: 0.0886\n",
      "Epoch 58/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1825 - _count_accuracy: 0.0886\n",
      "Epoch 59/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1807 - _count_accuracy: 0.0890\n",
      "Epoch 60/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.1806 - _count_accuracy: 0.0891\n",
      "Epoch 61/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1805 - _count_accuracy: 0.0892\n",
      "Epoch 62/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.1798 - _count_accuracy: 0.0894\n",
      "Epoch 63/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1795 - _count_accuracy: 0.0895\n",
      "Epoch 64/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.1801 - _count_accuracy: 0.0896\n",
      "Epoch 65/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1797 - _count_accuracy: 0.0897\n",
      "Epoch 66/100\n",
      "433/433 [==============================] - 112s 258ms/step - loss: 0.1776 - _count_accuracy: 0.0901\n",
      "Epoch 67/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1754 - _count_accuracy: 0.0907\n",
      "Epoch 68/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1752 - _count_accuracy: 0.0908\n",
      "Epoch 69/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1737 - _count_accuracy: 0.0912\n",
      "Epoch 70/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1729 - _count_accuracy: 0.0913\n",
      "Epoch 71/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1716 - _count_accuracy: 0.0917\n",
      "Epoch 72/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1713 - _count_accuracy: 0.0918\n",
      "Epoch 73/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1696 - _count_accuracy: 0.0922\n",
      "Epoch 74/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1695 - _count_accuracy: 0.0923\n",
      "Epoch 75/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1679 - _count_accuracy: 0.0927\n",
      "Epoch 76/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1677 - _count_accuracy: 0.0927\n",
      "Epoch 77/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1662 - _count_accuracy: 0.0930\n",
      "Epoch 78/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1662 - _count_accuracy: 0.0931\n",
      "Epoch 79/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1645 - _count_accuracy: 0.0936\n",
      "Epoch 80/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1650 - _count_accuracy: 0.0935\n",
      "Epoch 81/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1635 - _count_accuracy: 0.0938\n",
      "Epoch 82/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1642 - _count_accuracy: 0.0937\n",
      "Epoch 83/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1620 - _count_accuracy: 0.0943\n",
      "Epoch 84/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1627 - _count_accuracy: 0.0940\n",
      "Epoch 85/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1617 - _count_accuracy: 0.0942\n",
      "Epoch 86/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1629 - _count_accuracy: 0.0939\n",
      "Epoch 87/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1614 - _count_accuracy: 0.0944\n",
      "Epoch 88/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1611 - _count_accuracy: 0.0945\n",
      "Epoch 89/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1604 - _count_accuracy: 0.0947\n",
      "Epoch 90/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1603 - _count_accuracy: 0.0948\n",
      "Epoch 91/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1592 - _count_accuracy: 0.0949\n",
      "Epoch 92/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1598 - _count_accuracy: 0.0948\n",
      "Epoch 93/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1587 - _count_accuracy: 0.0951\n",
      "Epoch 94/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1589 - _count_accuracy: 0.0950\n",
      "Epoch 95/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1591 - _count_accuracy: 0.0950\n",
      "Epoch 96/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1592 - _count_accuracy: 0.0950\n",
      "Epoch 97/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1584 - _count_accuracy: 0.0952\n",
      "Epoch 98/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1571 - _count_accuracy: 0.0954\n",
      "Epoch 99/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1581 - _count_accuracy: 0.0953\n",
      "Epoch 100/100\n",
      "433/433 [==============================] - 111s 257ms/step - loss: 0.1578 - _count_accuracy: 0.0954\n"
     ]
    }
   ],
   "source": [
    "tf.fit(path='data/conv-ai-2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_models/transformer/conv-ai-2017/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_models/transformer/conv-ai-2017/assets\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x0000025578332E00> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x00000255783EE7A0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x00000255796F4C10> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x000002557970E830> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x00000255796DAFB0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x000002557977D840> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x00000256C3226470> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x00000256C3225330> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x00000256C32BEC50> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x00000256C3292C50> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x00000256C3331D20> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x00000256C3366E90> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x00000256C3365A80> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x00000256C43D1180> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x00000256C33675E0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x00000256C444AD40> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x00000256C43F2DA0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<chatbot.transformer.MultiHeadAttention object at 0x00000256C43F2320> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'chatbot.transformer.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "tf.save_to_folder(path='trained_models/transformer/conv-ai-2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN\n",
    "\n",
    "*Bag-Of-Words based DNN network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.stem.lancaster import LancasterStemmer\n",
    "# import numpy as np\n",
    "# import tflearn\n",
    "# import tensorflow as tf\n",
    "# import json\n",
    "# import pickle\n",
    "# import random\n",
    "\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "# #Loading intents.json\n",
    "# with open('intents.json') as intents:\n",
    "#   data = json.load(intents)\n",
    "\n",
    "# stemmer = LancasterStemmer()\n",
    "\n",
    "# try:\n",
    "#     with open('data.pickle','rb') as f:\n",
    "#         words, labels, training, output = pickle.load(f)\n",
    "# except:\n",
    "# # Fetching and Feeding information--\n",
    "#     words = []\n",
    "#     labels = []\n",
    "#     x_docs = []\n",
    "#     y_docs = []\n",
    "\n",
    "#     for intent in data['intents']:\n",
    "#         for pattern in intent['patterns']:\n",
    "#             wrds = nltk.word_tokenize(pattern)\n",
    "#             words.extend(wrds)\n",
    "#             x_docs.append(wrds)\n",
    "#             y_docs.append(intent['tag'])\n",
    "\n",
    "#             if intent['tag'] not in labels:\n",
    "#                 labels.append(intent['tag'])\n",
    "\n",
    "#     words = [stemmer.stem(w.lower()) for w in words if w not in \"?\"]\n",
    "#     words = sorted(list(set(words)))\n",
    "#     labels = sorted(labels)\n",
    "\n",
    "#     training = []\n",
    "#     output = []\n",
    "\n",
    "#     out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "#     # One hot encoding, Converting the words to numerals\n",
    "#     for x, doc in enumerate(x_docs):\n",
    "#         bag = []\n",
    "#         wrds = [stemmer.stem(w) for w in doc]\n",
    "#         for w in words:\n",
    "#             if w in wrds:\n",
    "#                 bag.append(1)\n",
    "#             else:\n",
    "#                 bag.append(0)\n",
    "\n",
    "\n",
    "#         output_row = out_empty[:]\n",
    "#         output_row[labels.index(y_docs[x])] = 1\n",
    "\n",
    "#         training.append(bag)\n",
    "#         output.append(output_row)\n",
    "\n",
    "\n",
    "#     training = np.array(training)\n",
    "#     output = np.array(output)\n",
    "\n",
    "#     with open('data.pickle','wb') as f:\n",
    "#         pickle.dump((words, labels, training, output), f)\n",
    "\n",
    "\n",
    "# net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "# net = tflearn.fully_connected(net, 8)\n",
    "# net = tflearn.fully_connected(net, 8)\n",
    "# net = tflearn.fully_connected(net, len(output[0]), activation='softmax')\n",
    "# net = tflearn.regression(net)\n",
    "\n",
    "# model = tflearn.DNN(net)\n",
    "\n",
    "# try:\n",
    "#     model.load(\"model.tflearn\")\n",
    "# except:\n",
    "\n",
    "#     model.fit(training, output, n_epoch=100, batch_size=8, show_metric=True)\n",
    "#     model.save('model.tflearn')\n",
    "\n",
    "\n",
    "# def bag_of_words(s, words):\n",
    "#     bag = [0 for _ in range(len(words))]\n",
    "#     s_words = nltk.word_tokenize(s)\n",
    "#     s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "#     for se in s_words:\n",
    "#         for i, w in enumerate(words):\n",
    "#             if w == se:\n",
    "#                 bag[i] = 1\n",
    "\n",
    "#     return np.array(bag)\n",
    "\n",
    "\n",
    "# def chat():\n",
    "#     print(\"The bot is ready to talk!!(Type 'quit' to exit)\")\n",
    "#     while True:\n",
    "#         inp = input(\"\\nYou: \")\n",
    "#         if inp.lower() == 'quit':\n",
    "#             break\n",
    "\n",
    "#     #Porbability of correct response \n",
    "#         results = model.predict([bag_of_words(inp, words)])\n",
    "\n",
    "#     # Picking the greatest number from probability\n",
    "#         results_index = np.argmax(results)\n",
    "\n",
    "#         tag = labels[results_index]\n",
    "\n",
    "\n",
    "#         for tg in data['intents']:\n",
    "\n",
    "#             if tg['tag'] == tag:\n",
    "#                 responses = tg['responses']\n",
    "#             print(\"Bot:\" + random.choice(responses))\n",
    "\n",
    "\n",
    "# chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tfenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "891e4fc61ac792e208938f292df2fbd7ee5dda42dce8831cfe4bd29bcfe9707e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
